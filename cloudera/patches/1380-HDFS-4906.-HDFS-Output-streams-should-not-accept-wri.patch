From 47a9c682ab6153677f0584db44f94497b6c8058d Mon Sep 17 00:00:00 2001
From: Aaron Twining Myers <atm@apache.org>
Date: Tue, 18 Jun 2013 21:08:37 +0000
Subject: [PATCH 1380/1475] HDFS-4906. HDFS Output streams should not accept writes after being closed. Contributed by Aaron T. Myers.

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/branch-2@1494304 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 1658c6bedc25f26b96754db146f87ed348a5bfd3)
---
 .../org/apache/hadoop/fs/ChecksumFileSystem.java   |   19 +++++-
 .../main/java/org/apache/hadoop/fs/ChecksumFs.java |   19 +++++-
 .../java/org/apache/hadoop/fs/FSOutputSummer.java  |   14 ++++-
 .../org/apache/hadoop/hdfs/DFSOutputStream.java    |   21 ++++---
 .../java/org/apache/hadoop/hdfs/TestClose.java     |   64 ++++++++++++++++++++
 .../hadoop/hdfs/TestMultiThreadedHflush.java       |   10 +--
 6 files changed, 124 insertions(+), 23 deletions(-)
 create mode 100644 hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClose.java

diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java
index 1770771..61cbe6a 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.fs;
 
 import java.io.*;
+import java.nio.channels.ClosedChannelException;
 import java.util.Arrays;
 
 import org.apache.commons.logging.Log;
@@ -364,6 +365,7 @@ public abstract class ChecksumFileSystem extends FilterFileSystem {
     private FSDataOutputStream datas;    
     private FSDataOutputStream sums;
     private static final float CHKSUM_AS_FRACTION = 0.01f;
+    private boolean isClosed = false;
     
     public ChecksumFSOutputSummer(ChecksumFileSystem fs, 
                           Path file, 
@@ -399,9 +401,13 @@ public abstract class ChecksumFileSystem extends FilterFileSystem {
     }
     
     public void close() throws IOException {
-      flushBuffer();
-      sums.close();
-      datas.close();
+      try {
+        flushBuffer();
+        sums.close();
+        datas.close();
+      } finally {
+        isClosed = true;
+      }
     }
     
     @Override
@@ -410,6 +416,13 @@ public abstract class ChecksumFileSystem extends FilterFileSystem {
       datas.write(b, offset, len);
       sums.write(checksum);
     }
+
+    @Override
+    protected void checkClosed() throws IOException {
+      if (isClosed) {
+        throw new ClosedChannelException();
+      }
+    }
   }
 
   /** {@inheritDoc} */
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java
index 4784991..561d74c 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java
@@ -20,6 +20,7 @@ package org.apache.hadoop.fs;
 
 import java.io.*;
 import java.net.URISyntaxException;
+import java.nio.channels.ClosedChannelException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.EnumSet;
@@ -319,6 +320,7 @@ public abstract class ChecksumFs extends FilterFs {
     private FSDataOutputStream datas;    
     private FSDataOutputStream sums;
     private static final float CHKSUM_AS_FRACTION = 0.01f;
+    private boolean isClosed = false;
     
     
     public ChecksumFSOutputSummer(final ChecksumFs fs, final Path file, 
@@ -349,9 +351,13 @@ public abstract class ChecksumFs extends FilterFs {
     }
     
     public void close() throws IOException {
-      flushBuffer();
-      sums.close();
-      datas.close();
+      try {
+        flushBuffer();
+        sums.close();
+        datas.close();
+      } finally {
+        isClosed = true;
+      }
     }
     
     @Override
@@ -360,6 +366,13 @@ public abstract class ChecksumFs extends FilterFs {
       datas.write(b, offset, len);
       sums.write(checksum);
     }
+
+    @Override
+    protected void checkClosed() throws IOException {
+      if (isClosed) {
+        throw new ClosedChannelException();
+      }
+    }
   }
 
   @Override
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSOutputSummer.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSOutputSummer.java
index 66b6a74..bc2961c 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSOutputSummer.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSOutputSummer.java
@@ -53,6 +53,15 @@ abstract public class FSOutputSummer extends OutputStream {
    */
   protected abstract void writeChunk(byte[] b, int offset, int len, byte[] checksum)
   throws IOException;
+  
+  /**
+   * Check if the implementing OutputStream is closed and should no longer
+   * accept writes. Implementations should do nothing if this stream is not
+   * closed, and should throw an {@link IOException} if it is closed.
+   * 
+   * @throws IOException if this stream is already closed.
+   */
+  protected abstract void checkClosed() throws IOException;
 
   /** Write one byte */
   public synchronized void write(int b) throws IOException {
@@ -82,7 +91,10 @@ abstract public class FSOutputSummer extends OutputStream {
    * @exception  IOException  if an I/O error occurs.
    */
   public synchronized void write(byte b[], int off, int len)
-  throws IOException {
+      throws IOException {
+    
+    checkClosed();
+    
     if (off < 0 || len < 0 || off > b.length - len) {
       throw new ArrayIndexOutOfBoundsException();
     }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
index 4f01ca2..120cfbc 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
@@ -30,6 +30,7 @@ import java.io.OutputStream;
 import java.net.InetSocketAddress;
 import java.net.Socket;
 import java.nio.BufferOverflowException;
+import java.nio.channels.ClosedChannelException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.EnumSet;
@@ -1259,10 +1260,10 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
     return sock;
   }
 
-  private void isClosed() throws IOException {
+  protected void checkClosed() throws IOException {
     if (closed) {
       IOException e = lastException;
-      throw e != null ? e : new IOException("DFSOutputStream is closed");
+      throw e != null ? e : new ClosedChannelException();
     }
   }
 
@@ -1420,7 +1421,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
           break;
         }
       }
-      isClosed();
+      checkClosed();
       queueCurrentPacket();
     }
   }
@@ -1430,7 +1431,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
   protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) 
                                                         throws IOException {
     dfsClient.checkOpen();
-    isClosed();
+    checkClosed();
 
     int cklen = checksum.length;
     int bytesPerChecksum = this.checksum.getBytesPerChecksum(); 
@@ -1541,7 +1542,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
 
   private void flushOrSync(boolean isSync) throws IOException {
     dfsClient.checkOpen();
-    isClosed();
+    checkClosed();
     try {
       long toWaitFor;
       synchronized (this) {
@@ -1619,7 +1620,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
           // If we got an error here, it might be because some other thread called
           // close before our hflush completed. In that case, we should throw an
           // exception that the stream is closed.
-          isClosed();
+          checkClosed();
           // If we aren't closed but failed to sync, we should expose that to the
           // caller.
           throw ioe;
@@ -1664,7 +1665,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
    */
   public synchronized int getCurrentBlockReplication() throws IOException {
     dfsClient.checkOpen();
-    isClosed();
+    checkClosed();
     if (streamer == null) {
       return blockReplication; // no pipeline, return repl factor of file
     }
@@ -1683,7 +1684,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
     long toWaitFor;
     synchronized (this) {
       dfsClient.checkOpen();
-      isClosed();
+      checkClosed();
       //
       // If there is data in the current buffer, send it across
       //
@@ -1700,7 +1701,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
     }
     synchronized (dataQueue) {
       while (!closed) {
-        isClosed();
+        checkClosed();
         if (lastAckedSeqno >= seqno) {
           break;
         }
@@ -1712,7 +1713,7 @@ public class DFSOutputStream extends FSOutputSummer implements Syncable {
         }
       }
     }
-    isClosed();
+    checkClosed();
   }
 
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClose.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClose.java
new file mode 100644
index 0000000..719743b
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClose.java
@@ -0,0 +1,64 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.nio.channels.ClosedChannelException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.test.GenericTestUtils;
+import org.junit.Test;
+
+public class TestClose {
+
+  @Test
+  public void testWriteAfterClose() throws IOException {
+    Configuration conf = new Configuration();
+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)
+        .build();
+    
+    try {
+      final byte[] data = "foo".getBytes();
+      
+      FileSystem fs = FileSystem.get(conf);
+      OutputStream out = fs.create(new Path("/test"));
+      
+      out.write(data);
+      out.close();
+      try {
+        // Should fail.
+        out.write(data);
+        fail("Should not have been able to write more data after file is closed.");
+      } catch (ClosedChannelException cce) {
+        // We got the correct exception. Ignoring.
+      }
+      // Should succeed. Double closes are OK.
+      out.close();
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+  
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java
index 5baca75..29c7f30 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hdfs;
 
 import java.io.IOException;
+import java.nio.channels.ClosedChannelException;
 import java.util.ArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
@@ -169,12 +170,9 @@ public class TestMultiThreadedHflush {
                 while (true) {
                   try {
                     stm.hflush();
-                  } catch (IOException ioe) {
-                    if (!ioe.toString().contains("DFSOutputStream is closed")) {
-                      throw ioe;
-                    } else {
-                      return;
-                    }
+                  } catch (ClosedChannelException ioe) {
+                    // Expected exception caught. Ignoring.
+                    return;
                   }
                 }
               } catch (Throwable t) {
-- 
1.7.0.4

